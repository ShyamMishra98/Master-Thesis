{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average word embeddings\n",
    "\n",
    "Applying the strength of word embeddings or vectors to larger text formats, such as documents or sentences, is a very basic technique in NLP.\n",
    "\n",
    "Suppose we have a sentence **T** , which is composed of words $w_{1}$, $w_{2}$, ⋯, $w_{n}$. Each word has a embedding $uw_{1}$, $uw_{2}$, ⋯, $uw_{n}$. So we define the sentence embedding as: $u_{\\mathbf{T}}$:= $\\frac{1}{n}$ $\\sum_{i=1}^{n}$ ${w_{u_{i}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoother = SmoothingFunction()\n",
    "from rouge.rouge import rouge_n_sentence_level # pip install easy-rouge\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\d072726\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\d072726\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for preprocessing\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained word embeddings\n",
    "\n",
    "- Download fasttext embeddings [here](https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec)\n",
    "- Download glove embeddings [here](http://nlp.stanford.edu/data/glove.840B.300d.zip)\n",
    "\n",
    "Unzip the glove embeddings and save the embeddings in a folder Pretrained_embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ../Pretrained_embeddings/wiki.en.vec\n",
      "INFO:gensim.models.utils_any2vec:loaded (2519370, 300) matrix from ../Pretrained_embeddings/wiki.en.vec\n"
     ]
    }
   ],
   "source": [
    "# for convience convert the fasttext and glove embeddings to word2vec format\n",
    "\n",
    "# for using fasttext embeddings\n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('../Pretrained_embeddings/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ../pretrained_embeddings/glove_word2vec.txt\n",
      "WARNING:gensim.models.utils_any2vec:duplicate word '����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������' in ../pretrained_embeddings/glove_word2vec.txt, ignoring all but first\n",
      "INFO:gensim.models.utils_any2vec:duplicate words detected, shrinking matrix size from 2196017 to 2196016\n",
      "INFO:gensim.models.utils_any2vec:loaded (2196016, 300) matrix from ../pretrained_embeddings/glove_word2vec.txt\n"
     ]
    }
   ],
   "source": [
    "# for using glove embeddings\n",
    "\n",
    "glove_file = '../Pretrained_embeddings/glove.840B.300d.txt'\n",
    "tmp_file = '../Pretrained_embeddings/glove_word2vec.txt'\n",
    "#_ = glove2word2vec(glove_file, tmp_file) # to convert glove to word2vec format and save it in tmp_file\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testsets for evaluation\n",
    "\n",
    "The Automatically generated candidate texts (predictions) from machine translation or text summarization are evaluated against their reference texts. <br> Below are the testsets to be used for evaluation. \n",
    "\n",
    "- For **DE-EN** translation, <br> **Candidate-**   '../Testsets/DE-EN/multi30k.test.pred.en.atok'  **Reference-**      '../Testsets/DE-EN/test2016.en.atok'    <br>\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, <br> **Candidate-**-   '../Testsets/RO-EN/newstest2016_output_1000.en'  **Reference-**    '../Testsets/RO-EN/newstest2016_ref_1000.en'  <br>\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, <br> **Candidate-**   '../Testsets/CNN-DM/preprocessed_1000.pred'  **Reference-** '../Testsets/CNN-DM/preprocessed_1000.ref'  \n",
    "\n",
    "\n",
    "- For **DUC2003** summarization, <br> **Candidate-**  '../Testsets/DUC2003/duc2003.10_300000-500.txt'  **Reference-** '../Testsets/DUC2003/task1_ref0_duc2003-500.txt'  \n",
    "\n",
    "\n",
    "- For **Gigaword** summarization (titles), <br>  **Candidate-**  '../Testsets/Gigaword/giga.10_300000_500.txt'  **Reference-** '../Testsets/Gigaword/task1_ref0_giga_500.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_doc =  '../Testsets/DE-EN/multi30k.test.pred.en.atok'  \n",
    "reference_doc = '../Testsets/DE-EN/test2016.en.atok' \n",
    "\n",
    "with  open( candidate_doc ,'r') as cand, open( reference_doc ,'r') as ref:\n",
    "    candidate_en = cand.readlines()\n",
    "    reference_en = ref.readlines()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man in an orange hat presenting something .\\n',\n",
       " 'A Boston traveler runs across lush , green fence in front of a white fence .\\n',\n",
       " 'A girl in a karate uniform is blocking a board with a kick .\\n',\n",
       " 'Five people in winter jackets and helmets are standing in the snow with vials in the background .\\n',\n",
       " 'People moving off the roof of a house .\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man in an orange hat starring at something .\\n',\n",
       " 'A Boston Terrier is running on lush green grass in front of a white fence .\\n',\n",
       " 'A girl in karate uniform breaking a stick with a front kick .\\n',\n",
       " 'Five people wearing winter jackets and helmets stand in the snow , with snowmobiles in the background .\\n',\n",
       " 'People are fixing the roof of a house .\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc, stop_words_remove=False):\n",
    "    remove_punctuation = []\n",
    "    preprocessed_doc = []\n",
    "    # keep only alphanumeric characters(remove punctuations)\n",
    "    remove_punctuation = [re.sub(r\"[^\\w]\", \" \", sent).lower().strip() for sent in doc] \n",
    "    \n",
    "    if stop_words_remove == True:\n",
    "        # remove stop words requires lower cased tokens\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        for sent in doc:\n",
    "            filtered_sentence = [word for word in word_tokenize(sent.lower()) if not word in stop_words]\n",
    "            preprocessed_doc.append(' '.join(filtered_sentence))\n",
    "        return preprocessed_doc\n",
    "    else:\n",
    "        return remove_punctuation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only if you want to preprocess the sentences\n",
    "\n",
    "candidate_en = preprocessing(candidate_en, False) # True to remove stopwords, default only removes punctuation\n",
    "reference_en = preprocessing(reference_en, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    # remove out-of-vocabulary words  \n",
    "    doc_tokenize = [word for word in doc.lower().split() if word in model.vocab]\n",
    "     \n",
    "    return np.mean(model[doc_tokenize], axis=0) # mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_embedding = []\n",
    "ref_embedding = []\n",
    "\n",
    "for doc in candidate_en:\n",
    "    cand_embedding.append(document_vector(fasttext_model, doc)) # glove_model for using glove embeddings\n",
    "for doc in reference_en:\n",
    "    ref_embedding.append(document_vector(fasttext_model, doc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity function\n",
    "\n",
    "semantic_scores =[]\n",
    "for i in range(len(cand_embedding)):\n",
    "    semantic_scores.append(np.dot(cand_embedding[i],ref_embedding[i]) / (np.linalg.norm(cand_embedding[i])*(np.linalg.norm(ref_embedding[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU or ROUGE scores\n",
    "\n",
    "Use BLEU scores for machine translation evaluation and ROUGE for text summarization evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for machine translation evaluation\n",
    "\n",
    "bleu_scores =[]\n",
    "for i in range(len(reference_en)):\n",
    "    bleu_scores.append(sentence_bleu(candidate_en[i],reference_en[i], smoothing_function=smoother.method4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text summarization evaluation\n",
    "\n",
    "rouge_scores = []\n",
    "for i in range(len(reference_en)):\n",
    "    *pr, f = rouge_n_sentence_level(candidate_en[i], reference_en[i], 1) # 2 for ROUGE-2. ROUGE-N, ROUGE-L and ROUGE-W scores can also be obtained.\n",
    "    rouge_scores.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human annotation scores\n",
    "\n",
    "Load the human annotation scores from the respective excel files as below,\n",
    "\n",
    "- For **DE-EN** translation, '../Human annotations/DE-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, '../Human annotations/RO-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, '../Human annotations/CNN_1000.xlsx'\n",
    "\n",
    "\n",
    "- For **DUC2003** summarization,  '../Human annotations/DUC2003.xlsx'\n",
    "\n",
    "\n",
    "- For **Gigaword** summarization (titles),  '../Human annotations/Gigaword.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotation = pd.read_excel('../Human annotations/DE-EN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = human_annotation.iloc[:, 2].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3901802069640419, 1.0390848166845472e-37)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between human annotated scores and Bleu or ROUGE scores\n",
    "\n",
    "#pearson correlation value, p-value\n",
    "\n",
    "pearsonr(human_scores, bleu_scores) # bleu_scores or rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6180671180926612, 2.048038569538953e-106)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between human annotated scores and semantic similarity scores\n",
    "\n",
    "pearsonr(human_scores, semantic_scores) # expected to be higher(more correlated) than with Bleu or ROUGE scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
