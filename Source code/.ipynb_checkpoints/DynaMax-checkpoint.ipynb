{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynaMax-Jaccard\n",
    "\n",
    "DynaMax-Jaccard is an unsupervised and non-parametric \n",
    "measure that dynamically extracts and max-pools good features to find the sentence representations depending on the sentence pair and similarity is measured using jaccard.\n",
    "\n",
    "It shows that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from vocabulary import Vocabulary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoother = SmoothingFunction()\n",
    "from rouge.rouge import rouge_n_sentence_level # pip install easy-rouge\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\d072726\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\d072726\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained word embeddings\n",
    "\n",
    "- Download fasttext embeddings [here](https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec)\n",
    "- Download glove embeddings [here](http://nlp.stanford.edu/data/glove.840B.300d.zip)\n",
    "\n",
    "Unzip the glove embeddings and save the embeddings in a folder pretrained_embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext = Vocabulary.from_embeddings('pretrained_embeddings/wiki.en.vec', pass_header=True) #pass_header, True for FastText\n",
    "glove = Vocabulary.from_embeddings('pretrained_embeddings/glove.840B.300d.txt', pass_header=False) #False for Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testsets for evaluation\n",
    "\n",
    "The Automatically generated texts (predictions) from machine translation or text summarization are evaluated against their reference texts. <br> Below are the testsets to be used for evaluation. \n",
    "\n",
    "- For **DE-EN** translation, <br>  **reference-** 'testsets/de-en/test2016.en.atok'   **prediction-** 'testsets/de-en/multi30k.test.pred.en.atok' <br>\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, <br>  **reference-** 'testsets/ro-en/newstest2016_ref_1000.en'  **prediction-**- 'testsets/ro-en/newstest2016_output_1000.en'<br>\n",
    "\n",
    "\n",
    "- For **giga word** summarization(titles), <br>  **reference-** 'testsets/giga/task1_ref0_giga_450.txt'  **prediction-**'testsets/giga/giga.10_300000_450.txt'\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, <br>  **reference-** 'testsets/cnn/preprocessed.ref'  **prediction-** 'testsets/cnn/preprocessed.pred'\n",
    "\n",
    "\n",
    "- For **Duc 2003** summarization, <br>  **reference-** 'testsets/duc/task1_ref0_duc2003.txt'  **prediction-** 'testsets/duc/duc2003.10_300000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_doc = 'testsets/de-en/test2016.en.atok'\n",
    "prediction_doc =  'testsets/de-en/multi30k.test.pred.en.atok'   \n",
    "\n",
    "with open( reference_doc ,'r') as ref, open( prediction_doc ,'r') as pred:\n",
    "    reference_en = ref.readlines()\n",
    "    prediction_en = pred.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc, stop_words_remove=False):\n",
    "    remove_punctuation = []\n",
    "    preprocessed_doc = []\n",
    "    # keep only alphanumeric characters(remove punctuations)\n",
    "    remove_punctuation = [re.sub(r\"[^\\w]\", \" \", sent).lower().strip() for sent in doc] \n",
    "    \n",
    "    if stop_words_remove == True:\n",
    "        # remove stop words requires lower cased tokens\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        for sent in doc:\n",
    "            filtered_sentence = [word for word in word_tokenize(sent.lower()) if not word in stop_words]\n",
    "            preprocessed_doc.append(' '.join(filtered_sentence))\n",
    "        return preprocessed_doc\n",
    "    else:\n",
    "        return remove_punctuation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only if you want to preprocess the sentences\n",
    "\n",
    "reference_en = preprocessing(reference_en, True) # True to remove stopwords, default only removes punctuation\n",
    "prediction_en = preprocessing(prediction_en, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings by aggregating the word embeddings\n",
    "def document_vector(doc):\n",
    "    doc_vec = []\n",
    "    for word in doc.lower().split():\n",
    "        if word in glove.word2id:  #glove for using glove embeddings\n",
    "            doc_vec.append(glove.embeddings[glove.word2id[word]])               \n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_embedding = []\n",
    "pred_embedding = []\n",
    "for doc in reference_en:\n",
    "    ref_embedding.append(np.array(document_vector(doc)))\n",
    "for doc in prediction_en:\n",
    "    pred_embedding.append(np.array(document_vector(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzify(s, u):\n",
    "    \"\"\"\n",
    "    Sentence fuzzifier.\n",
    "    Computes membership vector for the sentence S with respect to the\n",
    "    universe U\n",
    "    :param s: list of word embeddings for the sentence\n",
    "    :param u: the universe matrix U with shape (K, d)\n",
    "    :return: membership vectors for the sentence\n",
    "    \"\"\"\n",
    "    f_s = np.dot(s, u.T)\n",
    "    m_s = np.max(f_s, axis=0)\n",
    "    m_s = np.maximum(m_s, 0, m_s)\n",
    "    return m_s\n",
    "\n",
    "\n",
    "def dynamax_jaccard(x, y):\n",
    "    \"\"\"\n",
    "    DynaMax-Jaccard similarity measure between two sentences\n",
    "    :param x: list of word embeddings for the first sentence\n",
    "    :param y: list of word embeddings for the second sentence\n",
    "    :return: similarity score between the two sentences\n",
    "    \"\"\"\n",
    "    u = np.vstack((x, y))\n",
    "    m_x = fuzzify(x, u)\n",
    "    m_y = fuzzify(y, u)\n",
    "\n",
    "    m_inter = np.sum(np.minimum(m_x, m_y))\n",
    "    m_union = np.sum(np.maximum(m_x, m_y))\n",
    "    return m_inter / m_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dynamax =[]\n",
    "for i in range(len(ref_embedding)):\n",
    "    similarity_dynamax.append(dynamax_jaccard(ref_embedding[i], pred_embedding[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU or ROUGE scores\n",
    "\n",
    "Use BLEU scores for machine translation evaluation and ROUGE for text summarization evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for machine translation evaluation\n",
    "bleu_scores =[]\n",
    "for i in range(len(reference_en)):\n",
    "    bleu_scores.append(sentence_bleu(reference_en[i],prediction_en[i], smoothing_function=smoother.method4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text summarization evaluation\n",
    "rouge_scores = []\n",
    "for i in range(len(reference_en)):\n",
    "    *pr, f = rouge_n_sentence_level(prediction_en[i], reference_en[i], 2) # 2 for ROUGE-2. ROUGE-N, ROUGE-L and ROUGE-W scores can also be obtained.\n",
    "    rouge_scores.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human annotation scores\n",
    "\n",
    "Load the human annotation scores from the respective excel files as below,\n",
    "\n",
    "- For **DE-EN** translation, 'human annotated/DE-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, 'human annotated/RO-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **giga word** summarization(titles),'human annotated/giga.xlsx'\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, 'human annotated/CNN_900.xlsx'\n",
    "\n",
    "\n",
    "- For **Duc 2003** summarization,  'human annotated/duc2003.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotation = pd.read_excel('human annotated/DE-EN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = human_annotation.iloc[:, 3].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between human annotated scores and Bleu or ROUGE scores\n",
    "\n",
    "#pearson correlation value, p-value\n",
    "pearsonr(human_scores, bleu_scores) #bleu_scores or rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6618757852238534, 4.663116473627758e-127)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between human annotated scores and semantic similarity scores\n",
    "\n",
    "pearsonr(human_scores, similarity_dynamax) # expected to be higher(more correlated) than with Bleu or ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
