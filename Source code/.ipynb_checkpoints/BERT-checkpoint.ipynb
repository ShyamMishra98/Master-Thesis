{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers or the BERT is a method of pre-training language representations, that is a general-purpose language  model is trained on a large pain text corpus, and then that model is used for other downstream NLP tasks. BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install bert server and client using commands below in cmd prompt or terminal\n",
    "\n",
    "pip install bert-serving-server  # server\n",
    "\n",
    "pip install bert-serving-client  # client, independent of `bert-serving-server`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the BERT pretrained embeddings -  [**BERT-Base, Uncased**](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the server with the pretrained model and keep it running (in that folder in the cmd prompt)\n",
    "\n",
    "bert-serving-start -model_dir pretrained_embeddings\\BERT\\uncased_L-12_H-768_A-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bert_serving.client import BertClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoother = SmoothingFunction()\n",
    "from rouge.rouge import rouge_n_sentence_level # pip install easy-rouge\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testsets for evaluation\n",
    "\n",
    "The Automatically generated texts (predictions) from machine translation or text summarization are evaluated against their reference texts. <br> Below are the testsets to be used for evaluation. \n",
    "\n",
    "- For **DE-EN** translation, <br>  **reference-** '../testsets/de-en/test2016.en.atok'   **prediction-** '../testsets/de-en/multi30k.test.pred.en.atok' <br>\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, <br>  **reference-** '../testsets/ro-en/newstest2016_ref_1000.en'  **prediction-**- '../testsets/ro-en/newstest2016_output_1000.en'<br>\n",
    "\n",
    "\n",
    "- For **giga word** summarization(titles), <br>  **reference-** '../testsets/giga/task1_ref0_giga_450.txt'  **prediction-**'../testsets/giga/giga.10_300000_450.txt'\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, <br>  **reference-** '../testsets/cnn/preprocessed_1000.ref'  **prediction-** '../testsets/cnn/preprocessed_1000.pred'\n",
    "\n",
    "\n",
    "- For **Duc 2003** summarization, <br>  **reference-** '../testsets/duc/task1_ref0_duc2003.txt'  **prediction-** '../testsets/duc/duc2003.10_300000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_doc = '../testsets/cnn/preprocessed_1000.ref'\n",
    "prediction_doc =  '../testsets/cnn/preprocessed_1000.pred'  \n",
    "\n",
    "with open( reference_doc ,'r') as ref, open( prediction_doc ,'r') as pred:\n",
    "    reference_en = ref.readlines()\n",
    "    prediction_en = pred.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc, stop_words_remove=False):\n",
    "    remove_punctuation = []\n",
    "    preprocessed_doc = []\n",
    "    # keep only alphanumeric characters(remove punctuations)\n",
    "    remove_punctuation = [re.sub(r\"[^\\w]\", \" \", sent).lower().strip() for sent in doc] \n",
    "    \n",
    "    if stop_words_remove == True:\n",
    "        # remove stop words requires lower cased tokens\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        for sent in doc:\n",
    "            filtered_sentence = [word for word in word_tokenize(sent.lower()) if not word in stop_words]\n",
    "            preprocessed_doc.append(' '.join(filtered_sentence))\n",
    "        return preprocessed_doc\n",
    "    else:\n",
    "        return remove_punctuation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only if you want to preprocess the sentences\n",
    "\n",
    "reference_en = preprocessing(reference_en, False) # True to remove stopwords, default only removes punctuation\n",
    "prediction_en = preprocessing(prediction_en, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_bert_ref = bc.encode(reference_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_bert_pred = bc.encode(prediction_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_bert =[]\n",
    "for i in range(len(embeding_bert_ref)):\n",
    "    similarity_bert.append(np.dot(embeding_bert_ref[i],embeding_bert_pred[i]) / (np.linalg.norm(embeding_bert_ref[i])*(np.linalg.norm(embeding_bert_pred[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU or ROUGE scores\n",
    "\n",
    "Use BLEU scores for machine translation evaluation and ROUGE for text summarization evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for machine translation evaluation\n",
    "bleu_scores =[]\n",
    "for i in range(len(reference_en)):\n",
    "    bleu_scores.append(sentence_bleu(reference_en[i],prediction_en[i], smoothing_function=smoother.method4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text summarization evaluation\n",
    "rouge_scores = []\n",
    "for i in range(len(reference_en)):\n",
    "    *pr, f = rouge_n_sentence_level(prediction_en[i], reference_en[i], 2) # 2 for ROUGE-2. ROUGE-N, ROUGE-L and ROUGE-W scores can also be obtained.\n",
    "    rouge_scores.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human annotation scores\n",
    "\n",
    "Load the human annotation scores from the respective excel files as below,\n",
    "\n",
    "- For **DE-EN** translation, '../human annotated/DE-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, '../human annotated/RO-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **giga word** summarization(titles),'../human annotated/giga.xlsx'\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, '../human annotated/CNN_1000.xlsx'\n",
    "\n",
    "\n",
    "- For **Duc 2003** summarization,  '../human annotated/duc2003.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotation = pd.read_excel('../human annotated/CNN_1000.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = human_annotation.iloc[:, 2].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14908203814975426, 2.190813725020308e-06)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between human annotated scores and Bleu or ROUGE scores\n",
    "\n",
    "#pearson correlation value, p-value\n",
    "pearsonr(human_scores, rouge_scores) #bleu_scores or rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09988277691376685, 0.001564036550240868)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between human annotated scores and semantic similarity scores\n",
    "\n",
    "pearsonr(human_scores, similarity_bert) # expected to be higher(more correlated) than with Bleu or ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
