{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average word embeddings\n",
    "\n",
    "Applying the strength of word embeddings or vectors to larger text formats, such as documents or sentences, is a very basic technique in NLP.\n",
    "\n",
    "Suppose we have a sentence **T** , which is composed of words $w_{1}$, $w_{2}$, ⋯, $w_{n}$. Each word has a embedding $uw_{1}$, $uw_{2}$, ⋯, $uw_{n}$. So we define the sentence embedding as: $u_{\\mathbf{T}}$:= $\\frac{1}{n}$ $\\sum_{i=1}^{n}$ ${w_{u_{i}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoother = SmoothingFunction()\n",
    "from rouge.rouge import rouge_n_sentence_level # pip install easy-rouge\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained word embeddings\n",
    "\n",
    "- Download fasttext embeddings [here](https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec)\n",
    "- Download glove embeddings [here](http://nlp.stanford.edu/data/glove.840B.300d.zip)\n",
    "\n",
    "Unzip the glove embeddings and save the embeddings in a folder Pretrained_embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeyedVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6b92e2d34f91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# for using fasttext embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfasttext_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Pretrained_embeddings/wiki.en.vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'KeyedVectors' is not defined"
     ]
    }
   ],
   "source": [
    "# for convience convert the fasttext and glove embeddings to word2vec format\n",
    "\n",
    "# for using fasttext embeddings\n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('../Pretrained_embeddings/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ../pretrained_embeddings/glove_word2vec.txt\n",
      "WARNING:gensim.models.utils_any2vec:duplicate word '����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������' in ../pretrained_embeddings/glove_word2vec.txt, ignoring all but first\n",
      "INFO:gensim.models.utils_any2vec:duplicate words detected, shrinking matrix size from 2196017 to 2196016\n",
      "INFO:gensim.models.utils_any2vec:loaded (2196016, 300) matrix from ../pretrained_embeddings/glove_word2vec.txt\n"
     ]
    }
   ],
   "source": [
    "# for using glove embeddings\n",
    "\n",
    "glove_file = '../Pretrained_embeddings/glove.840B.300d.txt'\n",
    "tmp_file = '../Pretrained_embeddings/glove_word2vec.txt'\n",
    "#_ = glove2word2vec(glove_file, tmp_file) # to convert glove to word2vec format and save it in tmp_file\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testsets for evaluation\n",
    "\n",
    "The Automatically generated candidate texts (predictions) from machine translation or text summarization are evaluated against their reference texts. <br> Below are the testsets to be used for evaluation. \n",
    "\n",
    "- For **DE-EN** translation, <br> **Candidate-**   '../Testsets/DE-EN/multi30k.test.pred.en.atok'  **Reference-**      '../Testsets/DE-EN/test2016.en.atok'    <br>\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, <br> **Candidate-**-   '../Testsets/RO-EN/newstest2016_output_1000.en'  **Reference-**    '../Testsets/RO-EN/newstest2016_ref_1000.en'  <br>\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, <br> **Candidate-**   '../Testsets/CNN-DM/preprocessed_1000.pred'  **Reference-** '../Testsets/CNN-DM/preprocessed_1000.ref'  \n",
    "\n",
    "\n",
    "- For **DUC2003** summarization, <br> **Candidate-**  '../Testsets/DUC2003/duc2003.10_300000-500.txt'  **Reference-** '../Testsets/DUC2003/task1_ref0_duc2003-500.txt'  \n",
    "\n",
    "\n",
    "- For **Gigaword** summarization (titles), <br>  **Candidate-**  '../Testsets/Gigaword/giga.10_300000_500.txt'  **Reference-** '../Testsets/Gigaword/task1_ref0_giga_500.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_doc =  '../Testsets/DE-EN/multi30k.test.pred.en.atok'  \n",
    "reference_doc = '../Testsets/DE-EN/test2016.en.atok' \n",
    "\n",
    "with  open( candidate_doc ,'r') as cand, open( reference_doc ,'r') as ref:\n",
    "    candidate_en = cand.readlines()\n",
    "    reference_en = ref.readlines()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man in an orange hat presenting something .\\n',\n",
       " 'A Boston traveler runs across lush , green fence in front of a white fence .\\n',\n",
       " 'A girl in a karate uniform is blocking a board with a kick .\\n',\n",
       " 'Five people in winter jackets and helmets are standing in the snow with vials in the background .\\n',\n",
       " 'People moving off the roof of a house .\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man in an orange hat starring at something .\\n',\n",
       " 'A Boston Terrier is running on lush green grass in front of a white fence .\\n',\n",
       " 'A girl in karate uniform breaking a stick with a front kick .\\n',\n",
       " 'Five people wearing winter jackets and helmets stand in the snow , with snowmobiles in the background .\\n',\n",
       " 'People are fixing the roof of a house .\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc, stop_words_remove=False):\n",
    "    remove_punctuation = []\n",
    "    preprocessed_doc = []\n",
    "    # keep only alphanumeric characters(remove punctuations)\n",
    "    remove_punctuation = [re.sub(r\"[^\\w]\", \" \", sent).lower().strip() for sent in doc] \n",
    "    \n",
    "    if stop_words_remove == True:\n",
    "        # remove stop words requires lower cased tokens\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        for sent in remove_punctuation:\n",
    "            filtered_sentence = [word for word in word_tokenize(sent.lower()) if not word in stop_words]\n",
    "            preprocessed_doc.append(' '.join(filtered_sentence))\n",
    "        return preprocessed_doc\n",
    "    else:\n",
    "        return remove_punctuation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only if you want to preprocess the sentences\n",
    "\n",
    "candidate_en = preprocessing(candidate_en, False) # True to remove stopwords, default only removes punctuation\n",
    "reference_en = preprocessing(reference_en, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    # remove out-of-vocabulary words  \n",
    "    doc_tokenize = [word for word in doc.lower().split() if word in model.vocab]\n",
    "     \n",
    "    return np.mean(model[doc_tokenize], axis=0) #mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_embedding = []\n",
    "ref_embedding = []\n",
    "\n",
    "for doc in candidate_en:\n",
    "    cand_embedding.append(document_vector(fasttext_model, doc)) # glove_model for using glove embeddings\n",
    "for doc in reference_en:\n",
    "    ref_embedding.append(document_vector(fasttext_model, doc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_scores =[]\n",
    "for i in range(len(cand_embedding)):\n",
    "    semantic_scores.append(np.dot(cand_embedding[i],ref_embedding[i]) / (np.linalg.norm(cand_embedding[i])*(np.linalg.norm(ref_embedding[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU or ROUGE scores\n",
    "\n",
    "Use BLEU scores for machine translation evaluation and ROUGE for text summarization evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for machine translation evaluation\n",
    "bleu_scores =[]\n",
    "for i in range(len(reference_en)):\n",
    "    bleu_scores.append(sentence_bleu(candidate_en[i],reference_en[i], smoothing_function=smoother.method4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7172413793103449,\n",
       " 0.6540880503144655,\n",
       " 0.8275862068965517,\n",
       " 0.6308724832214765,\n",
       " 0.7272727272727272,\n",
       " 0.8266666666666667,\n",
       " 0.7058823529411765,\n",
       " 0.7200000000000001,\n",
       " 0.7804878048780488,\n",
       " 0.7407407407407407,\n",
       " 0.5964912280701755,\n",
       " 0.8181818181818182,\n",
       " 0.676470588235294,\n",
       " 0.7305389221556886,\n",
       " 0.7672955974842768,\n",
       " 0.6666666666666666,\n",
       " 0.6758620689655171,\n",
       " 0.6944444444444443,\n",
       " 0.8235294117647058,\n",
       " 0.7462686567164178,\n",
       " 0.6716417910447762,\n",
       " 0.543046357615894,\n",
       " 0.64,\n",
       " 0.7445255474452555,\n",
       " 0.7704918032786884,\n",
       " 0.6201550387596899,\n",
       " 0.6229508196721311,\n",
       " 0.672,\n",
       " 0.6065573770491803,\n",
       " 0.8051948051948051,\n",
       " 0.7857142857142857,\n",
       " 0.7424242424242424,\n",
       " 0.684931506849315,\n",
       " 0.4142857142857143,\n",
       " 0.6805555555555557,\n",
       " 0.7468354430379746,\n",
       " 0.681159420289855,\n",
       " 0.7230769230769231,\n",
       " 0.5095541401273885,\n",
       " 0.5045045045045046,\n",
       " 0.75,\n",
       " 0.7246376811594202,\n",
       " 0.5801526717557253,\n",
       " 0.7195121951219512,\n",
       " 0.6890756302521008,\n",
       " 0.5755395683453238,\n",
       " 0.6708860759493671,\n",
       " 0.7042253521126761,\n",
       " 0.704225352112676,\n",
       " 0.6764705882352942,\n",
       " 0.6754966887417219,\n",
       " 0.8074534161490684,\n",
       " 0.6621621621621621,\n",
       " 0.6622516556291391,\n",
       " 0.7083333333333334,\n",
       " 0.6074074074074074,\n",
       " 0.6567164179104478,\n",
       " 0.7450980392156863,\n",
       " 0.7891156462585034,\n",
       " 0.7234042553191489,\n",
       " 0.717948717948718,\n",
       " 0.7333333333333335,\n",
       " 0.4999999999999999,\n",
       " 0.6842105263157895,\n",
       " 0.640625,\n",
       " 0.7006369426751593,\n",
       " 0.715151515151515,\n",
       " 0.8533333333333333,\n",
       " 0.6774193548387097,\n",
       " 0.6842105263157895,\n",
       " 0.7518796992481203,\n",
       " 0.6201550387596898,\n",
       " 0.7310344827586207,\n",
       " 0.5961538461538463,\n",
       " 0.7642276422764227,\n",
       " 0.6015037593984963,\n",
       " 0.5436893203883495,\n",
       " 0.5565217391304348,\n",
       " 0.4881889763779528,\n",
       " 0.56,\n",
       " 0.8,\n",
       " 0.7971014492753622,\n",
       " 0.5238095238095238,\n",
       " 0.7205882352941178,\n",
       " 0.7594936708860761,\n",
       " 0.7428571428571429,\n",
       " 0.6753246753246754,\n",
       " 0.7299270072992701,\n",
       " 0.6618705035971223,\n",
       " 0.656,\n",
       " 0.5857142857142857,\n",
       " 0.6923076923076923,\n",
       " 0.8029197080291971,\n",
       " 0.6324786324786326,\n",
       " 0.637037037037037,\n",
       " 0.6666666666666667,\n",
       " 0.7162162162162162,\n",
       " 0.5950413223140496,\n",
       " 0.564516129032258,\n",
       " 0.7453416149068323,\n",
       " 0.7540983606557378,\n",
       " 0.6666666666666667,\n",
       " 0.7354838709677418,\n",
       " 0.7320261437908497,\n",
       " 0.782051282051282,\n",
       " 0.7152317880794701,\n",
       " 0.7938931297709925,\n",
       " 0.5862068965517242,\n",
       " 0.625,\n",
       " 0.6290322580645161,\n",
       " 0.6241134751773049,\n",
       " 0.7714285714285715,\n",
       " 0.6942148760330578,\n",
       " 0.7244094488188975,\n",
       " 0.6470588235294118,\n",
       " 0.5882352941176471,\n",
       " 0.6554621848739496,\n",
       " 0.8057553956834532,\n",
       " 0.7012987012987013,\n",
       " 0.6923076923076924,\n",
       " 0.7435897435897436,\n",
       " 0.6991869918699187,\n",
       " 0.6482758620689656,\n",
       " 0.7600000000000001,\n",
       " 0.611111111111111,\n",
       " 0.759124087591241,\n",
       " 0.7162162162162163,\n",
       " 0.8225806451612904,\n",
       " 0.7945205479452055,\n",
       " 0.7656250000000001,\n",
       " 0.8000000000000002,\n",
       " 0.7272727272727272,\n",
       " 0.5982905982905983,\n",
       " 0.6258503401360543,\n",
       " 0.6268656716417911,\n",
       " 0.7154471544715447,\n",
       " 0.7118644067796611,\n",
       " 0.6370370370370371,\n",
       " 0.7228915662650602,\n",
       " 0.7681159420289855,\n",
       " 0.6393442622950819,\n",
       " 0.7435897435897436,\n",
       " 0.676470588235294,\n",
       " 0.734375,\n",
       " 0.713375796178344,\n",
       " 0.7022900763358778,\n",
       " 0.8227848101265822,\n",
       " 0.7142857142857143,\n",
       " 0.7076923076923077,\n",
       " 0.6818181818181819,\n",
       " 0.751592356687898,\n",
       " 0.5396825396825397,\n",
       " 0.7007299270072992,\n",
       " 0.7643312101910827,\n",
       " 0.6906474820143885,\n",
       " 0.8092485549132947,\n",
       " 0.7338129496402879,\n",
       " 0.7152317880794701,\n",
       " 0.7073170731707318,\n",
       " 0.728395061728395,\n",
       " 0.736196319018405,\n",
       " 0.7402597402597403,\n",
       " 0.8025477707006369,\n",
       " 0.7534246575342466,\n",
       " 0.7757575757575759,\n",
       " 0.7401574803149606,\n",
       " 0.6379310344827586,\n",
       " 0.7000000000000001,\n",
       " 0.6504065040650407,\n",
       " 0.7034482758620688,\n",
       " 0.7131782945736433,\n",
       " 0.6814814814814814,\n",
       " 0.6878980891719745,\n",
       " 0.5633802816901408,\n",
       " 0.6951219512195121,\n",
       " 0.7448275862068965,\n",
       " 0.7999999999999999,\n",
       " 0.6455696202531646,\n",
       " 0.7749999999999999,\n",
       " 0.5174825174825175,\n",
       " 0.6933333333333334,\n",
       " 0.6933333333333334,\n",
       " 0.6913580246913581,\n",
       " 0.7763157894736843,\n",
       " 0.5899280575539568,\n",
       " 0.5899280575539567,\n",
       " 0.5535714285714286,\n",
       " 0.8258064516129032,\n",
       " 0.6845637583892618,\n",
       " 0.7931034482758621,\n",
       " 0.6935483870967741,\n",
       " 0.6299212598425198,\n",
       " 0.6842105263157894,\n",
       " 0.6742857142857143,\n",
       " 0.6833333333333333,\n",
       " 0.6142857142857142,\n",
       " 0.6557377049180327,\n",
       " 0.6666666666666667,\n",
       " 0.5853658536585366,\n",
       " 0.736842105263158,\n",
       " 0.6754966887417219,\n",
       " 0.5909090909090908,\n",
       " 0.751592356687898,\n",
       " 0.7297297297297296,\n",
       " 0.694214876033058,\n",
       " 0.6046511627906976,\n",
       " 0.7260273972602739,\n",
       " 0.794701986754967,\n",
       " 0.6911764705882354,\n",
       " 0.7638888888888887,\n",
       " 0.7999999999999999,\n",
       " 0.7577639751552796,\n",
       " 0.7042253521126761,\n",
       " 0.6721311475409837,\n",
       " 0.7272727272727272,\n",
       " 0.6190476190476191,\n",
       " 0.8322147651006712,\n",
       " 0.6470588235294118,\n",
       " 0.7417218543046358,\n",
       " 0.7846153846153847,\n",
       " 0.6962962962962963,\n",
       " 0.6029411764705882,\n",
       " 0.7625899280575541,\n",
       " 0.7175572519083969,\n",
       " 0.7448275862068966,\n",
       " 0.6474820143884893,\n",
       " 0.6762589928057554,\n",
       " 0.7906976744186046,\n",
       " 0.7445255474452555,\n",
       " 0.7183098591549296,\n",
       " 0.7428571428571428,\n",
       " 0.8679245283018868,\n",
       " 0.7733333333333333,\n",
       " 0.8205128205128205,\n",
       " 0.7712418300653595,\n",
       " 0.825,\n",
       " 0.7875,\n",
       " 0.7792207792207793,\n",
       " 0.7692307692307693,\n",
       " 0.7435897435897435,\n",
       " 0.72992700729927,\n",
       " 0.6724137931034482,\n",
       " 0.5,\n",
       " 0.7769784172661871,\n",
       " 0.5203252032520325,\n",
       " 0.8120300751879698,\n",
       " 0.8,\n",
       " 0.6399999999999999,\n",
       " 0.6428571428571428,\n",
       " 0.6901408450704225,\n",
       " 0.6046511627906976,\n",
       " 0.7006369426751593,\n",
       " 0.75,\n",
       " 0.7547169811320754,\n",
       " 0.7183098591549295,\n",
       " 0.7450980392156864,\n",
       " 0.75,\n",
       " 0.7234042553191489,\n",
       " 0.689655172413793,\n",
       " 0.7432432432432431,\n",
       " 0.7421383647798743,\n",
       " 0.7701863354037266,\n",
       " 0.6950354609929078,\n",
       " 0.6923076923076924,\n",
       " 0.6330935251798562,\n",
       " 0.6573426573426574,\n",
       " 0.6277372262773723,\n",
       " 0.5755395683453237,\n",
       " 0.7714285714285715,\n",
       " 0.725925925925926,\n",
       " 0.6356589147286822,\n",
       " 0.6666666666666665,\n",
       " 0.5873015873015872,\n",
       " 0.6458333333333334,\n",
       " 0.7176470588235294,\n",
       " 0.5669291338582677,\n",
       " 0.6915887850467289,\n",
       " 0.6423357664233577,\n",
       " 0.7445255474452556,\n",
       " 0.5895953757225434,\n",
       " 0.6423357664233577,\n",
       " 0.6034482758620691,\n",
       " 0.6554621848739497,\n",
       " 0.732824427480916,\n",
       " 0.4230769230769231,\n",
       " 0.7218045112781953,\n",
       " 0.624,\n",
       " 0.5087719298245613,\n",
       " 0.706766917293233,\n",
       " 0.7343749999999999,\n",
       " 0.6559999999999999,\n",
       " 0.5504587155963303,\n",
       " 0.7162162162162163,\n",
       " 0.7692307692307692,\n",
       " 0.7205882352941176,\n",
       " 0.6456692913385828,\n",
       " 0.6666666666666666,\n",
       " 0.6206896551724138,\n",
       " 0.6853146853146853,\n",
       " 0.6356589147286822,\n",
       " 0.7530864197530865,\n",
       " 0.6013986013986015,\n",
       " 0.6518518518518519,\n",
       " 0.6612903225806451,\n",
       " 0.53125,\n",
       " 0.7092198581560283,\n",
       " 0.6031746031746031,\n",
       " 0.6330935251798562,\n",
       " 0.6456692913385826,\n",
       " 0.7605633802816901,\n",
       " 0.7256637168141592,\n",
       " 0.835820895522388,\n",
       " 0.6825396825396827,\n",
       " 0.6845637583892616,\n",
       " 0.6666666666666666,\n",
       " 0.6993006993006993,\n",
       " 0.5942028985507247,\n",
       " 0.5217391304347826,\n",
       " 0.59375,\n",
       " 0.4173913043478261,\n",
       " 0.5833333333333334,\n",
       " 0.59375,\n",
       " 0.582089552238806,\n",
       " 0.5692307692307692,\n",
       " 0.6271186440677966,\n",
       " 0.6623376623376623,\n",
       " 0.7189542483660132,\n",
       " 0.8375,\n",
       " 0.7602339181286549,\n",
       " 0.8433734939759037,\n",
       " 0.7260273972602739,\n",
       " 0.7702702702702703,\n",
       " 0.7151515151515152,\n",
       " 0.6962025316455697,\n",
       " 0.7230769230769231,\n",
       " 0.7870967741935484,\n",
       " 0.7320261437908496,\n",
       " 0.7712418300653594,\n",
       " 0.6956521739130435,\n",
       " 0.7751937984496124,\n",
       " 0.7338129496402879,\n",
       " 0.6956521739130435,\n",
       " 0.782608695652174,\n",
       " 0.6829268292682926,\n",
       " 0.7142857142857142,\n",
       " 0.7741935483870969,\n",
       " 0.7919463087248322,\n",
       " 0.8400000000000001,\n",
       " 0.75,\n",
       " 0.64,\n",
       " 0.7169811320754716,\n",
       " 0.735483870967742,\n",
       " 0.7058823529411765,\n",
       " 0.7034482758620689,\n",
       " 0.6363636363636364,\n",
       " 0.7974683544303797,\n",
       " 0.6060606060606061,\n",
       " 0.7272727272727273,\n",
       " 0.7716535433070867,\n",
       " 0.6422018348623854,\n",
       " 0.5846153846153846,\n",
       " 0.6071428571428571,\n",
       " 0.6,\n",
       " 0.7692307692307693,\n",
       " 0.6870229007633589,\n",
       " 0.7142857142857143,\n",
       " 0.8053691275167785,\n",
       " 0.6622516556291392,\n",
       " 0.6712328767123287,\n",
       " 0.5528455284552846,\n",
       " 0.7034482758620688,\n",
       " 0.6716417910447761,\n",
       " 0.7101449275362318,\n",
       " 0.6153846153846153,\n",
       " 0.6153846153846153,\n",
       " 0.6710526315789473,\n",
       " 0.7812499999999999,\n",
       " 0.7333333333333334,\n",
       " 0.7536231884057971,\n",
       " 0.7777777777777778,\n",
       " 0.7027027027027027,\n",
       " 0.6890756302521008,\n",
       " 0.7603305785123967,\n",
       " 0.75,\n",
       " 0.6618705035971223,\n",
       " 0.6883116883116883,\n",
       " 0.7724137931034483,\n",
       " 0.6792452830188679,\n",
       " 0.7172413793103448,\n",
       " 0.6099290780141844,\n",
       " 0.732919254658385,\n",
       " 0.6582278481012658,\n",
       " 0.7402597402597403,\n",
       " 0.618421052631579,\n",
       " 0.7388535031847134,\n",
       " 0.6950354609929077,\n",
       " 0.6883116883116882,\n",
       " 0.8153846153846154,\n",
       " 0.6911764705882353,\n",
       " 0.7755102040816326,\n",
       " 0.703030303030303,\n",
       " 0.7792207792207794,\n",
       " 0.6857142857142857,\n",
       " 0.7891156462585034,\n",
       " 0.7826086956521738,\n",
       " 0.7848101265822784,\n",
       " 0.7412587412587412,\n",
       " 0.7246376811594203,\n",
       " 0.6106870229007635,\n",
       " 0.7407407407407408,\n",
       " 0.7499999999999999,\n",
       " 0.6712328767123288,\n",
       " 0.6324786324786325,\n",
       " 0.6887417218543046,\n",
       " 0.7483870967741935,\n",
       " 0.7943262411347518,\n",
       " 0.7974683544303797,\n",
       " 0.7974683544303797,\n",
       " 0.7599999999999999,\n",
       " 0.7999999999999999,\n",
       " 0.7999999999999999,\n",
       " 0.7843137254901961,\n",
       " 0.7951807228915662,\n",
       " 0.7368421052631579,\n",
       " 0.8715083798882681,\n",
       " 0.8715083798882681,\n",
       " 0.7297297297297298,\n",
       " 0.7341772151898734,\n",
       " 0.7857142857142858,\n",
       " 0.748201438848921,\n",
       " 0.7611940298507464,\n",
       " 0.7751937984496123,\n",
       " 0.7076923076923077,\n",
       " 0.6056338028169014,\n",
       " 0.6564885496183206,\n",
       " 0.7153284671532847,\n",
       " 0.75,\n",
       " 0.8258064516129032,\n",
       " 0.8258064516129032,\n",
       " 0.7236842105263158,\n",
       " 0.7445255474452555,\n",
       " 0.757142857142857,\n",
       " 0.8057553956834532,\n",
       " 0.7517730496453902,\n",
       " 0.6562499999999999,\n",
       " 0.7432432432432431,\n",
       " 0.6714285714285714,\n",
       " 0.7320261437908497,\n",
       " 0.583941605839416,\n",
       " 0.7034482758620689,\n",
       " 0.7453416149068322,\n",
       " 0.822857142857143,\n",
       " 0.6845637583892618,\n",
       " 0.830188679245283,\n",
       " 0.7058823529411765,\n",
       " 0.765625,\n",
       " 0.6774193548387096,\n",
       " 0.6779661016949152,\n",
       " 0.6779661016949152,\n",
       " 0.7814569536423841,\n",
       " 0.728395061728395,\n",
       " 0.6379310344827587,\n",
       " 0.696969696969697,\n",
       " 0.6466165413533834,\n",
       " 0.6307692307692309,\n",
       " 0.6891891891891891,\n",
       " 0.8299319727891157,\n",
       " 0.7777777777777779,\n",
       " 0.8533333333333333,\n",
       " 0.8088235294117647,\n",
       " 0.8527131782945736,\n",
       " 0.7808219178082193,\n",
       " 0.7313432835820897,\n",
       " 0.7320261437908497,\n",
       " 0.7910447761194029,\n",
       " 0.7482014388489209,\n",
       " 0.8333333333333334,\n",
       " 0.8129032258064516,\n",
       " 0.6577181208053692,\n",
       " 0.6986301369863014,\n",
       " 0.6474820143884892,\n",
       " 0.7832167832167831,\n",
       " 0.7534246575342466,\n",
       " 0.7547169811320755,\n",
       " 0.6762589928057554,\n",
       " 0.736842105263158,\n",
       " 0.6482758620689656,\n",
       " 0.6710526315789473,\n",
       " 0.7111111111111111,\n",
       " 0.7176470588235294,\n",
       " 0.6918238993710693,\n",
       " 0.6577181208053691,\n",
       " 0.728476821192053,\n",
       " 0.7142857142857142,\n",
       " 0.6296296296296297,\n",
       " 0.7462686567164178,\n",
       " 0.7131782945736435,\n",
       " 0.7313432835820897,\n",
       " 0.7205882352941175,\n",
       " 0.7301587301587301,\n",
       " 0.6727272727272727,\n",
       " 0.7818181818181819]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text summarization evaluation\n",
    "rouge_scores = []\n",
    "for i in range(len(reference_en)):\n",
    "    *pr, f = rouge_n_sentence_level(candidate_en[i], reference_en[i], 1) # 2 for ROUGE-2. ROUGE-N, ROUGE-L and ROUGE-W scores can also be obtained.\n",
    "    rouge_scores.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human annotation scores\n",
    "\n",
    "Load the human annotation scores from the respective excel files as below,\n",
    "\n",
    "- For **DE-EN** translation, '../Human annotations/DE-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **RO-EN** translation, '../Human annotations/RO-EN.xlsx'\n",
    "\n",
    "\n",
    "- For **CNN-DM** summariation, '../Human annotations/CNN_1000.xlsx'\n",
    "\n",
    "\n",
    "- For **DUC2003** summarization,  '../Human annotations/DUC2003.xlsx'\n",
    "\n",
    "\n",
    "- For **Gigaword** summarization (titles),  '../Human annotations/Gigaword.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotation = pd.read_excel('../human annotated/DE-EN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = human_annotation.iloc[:, 2].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rouge_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c5d2ad57896d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#pearson correlation value, p-value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpearsonr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge_scores\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#bleu_scores or rouge_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rouge_scores' is not defined"
     ]
    }
   ],
   "source": [
    "# correlation between human annotated scores and Bleu or ROUGE scores\n",
    "\n",
    "#pearson correlation value, p-value\n",
    "pearsonr(human_scores, rouge_scores) #bleu_scores or rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20064031473151583, 6.146642639596484e-06)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between human annotated scores and semantic similarity scores\n",
    "\n",
    "pearsonr(human_scores, semantic_scores) # expected to be higher(more correlated) than with Bleu or ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
